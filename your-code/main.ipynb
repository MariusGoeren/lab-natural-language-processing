{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"../data/kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL...\n",
       "1                                             Will do.\n",
       "2    Nora--Cheryl has emailed dozens of memos about...\n",
       "3    Dear Sir=2FMadam=2C I know that this proposal ...\n",
       "4                                                  fyi\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_inline_css_js(text: str) -> str:\n",
    "\n",
    "    # Remove inline JavaScript/CSS and strip HTML to plain text.\n",
    "    # Works with both escaped tags (&lt;tag&gt;) and real tags (<tag>).\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # If the input contains &lt;...&gt; entities, convert them to real <...> first.\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # 1) Remove <script>...</script> blocks\n",
    "    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # 2) Remove <style>...</style> blocks\n",
    "    text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # 3) Remove inline CSS attributes: style=\"...\"\n",
    "    text = re.sub(r'\\s*style=\"[^\"]*\"', '', text)\n",
    "\n",
    "    # 4) Remove HTML comments <!-- ... -->\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 5) Strip the remaining HTML tags (<div>, <p>, <br>, etc.)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # 6) Remove leftover broken tag fragments like \"STRONG>\" or \"/DIV>\"\n",
    "    text = re.sub(r'/?[A-Za-z]+>', '', text)\n",
    "\n",
    "    # 7) Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example: apply to the X_test and X_train data\n",
    "\n",
    "X_test = X_test.fillna(\"\").astype(str).apply(remove_inline_css_js) \n",
    "X_train = X_train.fillna(\"\").astype(str).apply(remove_inline_css_js)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DEAR SIR, STRICTLY A PRIVATE BUSINESS PROPOSAL I AM MIKE CHUKWU , THE MANAGER, BILLS AND EXCHANGE AT THE FOREIGN REMITTANCE DEPARTMENT OF THE ZENITH INTERNATIONAL BANK PLC. I AM WRITING THIS LETTER TO ASK FOR YOUR SUPPORT AND COOPERATION TO CARRY OUT THIS BUSINESS OPPORTUNITY IN MY DEPARTMENT. WE DISCOVERED AN ABANDONED SUM OF $15,000,000.00 (FIFTEEN MILLION UNITED STATES DOLLARS ONLY) IN AN ACCOUNT THAT BELONGS TO ONE OF OUR FOREIGN CUSTOMERS WHO DIED ALONG WITH HIS ENTIRE FAMILY OF A WIFE AND TWO CHILDREN IN NOVEMBER 1997 IN A PLANE CRASH. SINCE WE HEARD OF HIS DEATH, WE HAVE BEEN EXPECTING HIS NEXT-OF-KIN TO COME OVER AND PUT CLAIMS FOR HIS MONEY AS THE HEIR,BECAUSE WE CANNOT RELEASE THE FUND FROM HIS ACCOUNT UNLESS SOMEONE APPLIES FOR CLAIM AS THE NEXT-OF-KIN TO THE DECEASED AS INDICATED IN OUR BANKING GUIDELINES. UNFORTUNATELY, NEITHER THEIR FAMILY MEMBER NOR DISTANT RELATIVE HAS EVER APPEARED TO CLAIM THE SAID FUND. UPON THIS DISCOVERY,I AND OTHER OFFICIALS IN MY DEPARTMENT HAVE AGREED TO MAKE BUSINESS WITH YOU AND RELEASE THE TOTAL AMOUNT INTO YOUR ACCOUNT AS THE HEIR OF THE FUND SINCE NO ONE CAME FOR IT OR DISCOVERED HE MAINTAINED ACCOUNT WITH OUR BANK, OTHERWISE THE FUND WILL BE RETURNED TO THE BANKS TREASURY AS UNCLAIMED FUND. WE HAVE AGREED THAT OUR RATIO OF SHARING WILL BE AS STATED THUS; 30 % FOR YOU AS FOREIGN PARTNER, 60 % FOR US THE OFFICIALS IN MY DEPARTMENT AND 10% FOR THE SETTLEMENT OF ALL LOCAL AND FOREIGN EXPENCES INCURRED BY US AND YOU DURING THE COURSE OF THIS BUSINESS. UPON THE SUCCESSFUL COMPLETION OF THIS TRANSFER, I AND ONE OF MY COLLEAGUES WILL COME TO YOUR COUNTRY AND MIND OUR SHARE. IT IS FROM OUR 60 % WE INTEND TO IMPORT AGRICULTURAL MACHINERIES INTO MY COUNTRY AS A WAY OF RECYCLING THE FUND. TO COMMENCE THIS TRANSACTION, WE REQUIRE YOU TO IMMEDIATELY INDICATE YOUR INTEREST BY A RETURN E-MAIL AND ENCLOSE YOUR PRIVATE CONTACT TELEPHONE NUMBER, FAX NUMBER FULL NAME AND ADDRESS AND YOUR DESIGNATED BANK COORDINATES TO ENABLE US FILE LETTER OF CLAIM TO THE APPROPRIATE DEPARTMENTS FOR NECESSARY APPROVALS BEFORE THE TRANSFER CAN BE MADE. NOTE ALSO, THIS TRANSACTION MUST BE KEPT STRICTLY CONFIDENTIAL BECAUSE OF IT'S NATURE. I LOOK FORWARD TO RECEIVING YOUR PROMPT RESPONSE. REGARDS, MR. MIKE CHUKWU ZENITH INTERNATIONAL BANK PLC .\""
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dear sir strictly private business proposal mike chukwu manager bills exchange foreign remittance department zenith international bank plc writing letter ask support cooperation carry business opportunity department discovered abandoned sum fifteen million united states dollars account belongs one foreign customers died along entire family wife two children november plane crash since heard death expecting nextofkin come put claims money heirbecause cannot release fund account unless someone applies claim nextofkin deceased indicated banking guidelines unfortunately neither family member distant relative ever appeared claim said fund upon discoveryi officials department agreed make business release total amount account heir fund since one came discovered maintained account bank otherwise fund returned banks treasury unclaimed fund agreed ratio sharing stated thus foreign partner us officials department settlement local foreign expences incurred us course business upon successful completion transfer one colleagues come country mind share intend import agricultural machineries country way recycling fund commence transaction require immediately indicate interest return email enclose private contact telephone number fax number full name address designated bank coordinates enable us file letter claim appropriate departments necessary approvals transfer made note also transaction must kept strictly confidential nature look forward receiving prompt response regards mr mike chukwu zenith international bank plc'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "#############################################################\n",
    "# Remove all the special characters\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "X_train = X_train.apply(remove_punctuation)\n",
    "X_test = X_test.apply(remove_punctuation)\n",
    "\n",
    "#############################################################\n",
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)                 # one or more digits\n",
    "\n",
    "X_train = X_train.apply(remove_numbers)\n",
    "X_test = X_test.apply(remove_numbers)\n",
    "\n",
    "#############################################################\n",
    "# Remove all single characters\n",
    "def remove_single_chars(text):\n",
    "    return re.sub(r\"(?<=\\s).(?=\\s)\", \"\", text)      # exactly one single character, if there is a space before and after this character\n",
    "    # return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)      # exactly one single letter\n",
    "\n",
    "X_train = X_train.apply(remove_single_chars)\n",
    "X_test = X_test.apply(remove_single_chars)\n",
    "\n",
    "#############################################################\n",
    "# Remove single characters from the start\n",
    "def remove_first_char(text):\n",
    "    return re.sub(r\"^(?=\\S\\s)\\S\", \"\", text)         # remove first char if it is followed by a space\n",
    "\n",
    "X_train = X_train.apply(remove_first_char)\n",
    "X_test = X_test.apply(remove_first_char)\n",
    "\n",
    "#############################################################\n",
    "# Substitute multiple spaces with single space\n",
    "\n",
    "def normalize_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()        # replace multiple spaces by only a single space. .striß -> Delete spaces at the beginning or the end\n",
    "\n",
    "X_train = X_train.apply(normalize_spaces)\n",
    "X_test = X_test.apply(normalize_spaces)\n",
    "\n",
    "#############################################################\n",
    "# Remove prefixed 'b'\n",
    "def remove_b_before_capital(text):\n",
    "    return re.sub(r\"b(?=[A-Z])\", \"\", text)          # delete \"b\" it is followed by a capital letter\n",
    "\n",
    "X_train = X_train.apply(remove_b_before_capital)\n",
    "X_test = X_test.apply(remove_b_before_capital)\n",
    "\n",
    "#############################################################\n",
    "# Convert to Lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "X_train = X_train.apply(to_lowercase)\n",
    "X_test = X_test.apply(to_lowercase)\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dear sir strictly private business proposal mike chukwu manager bills exchange foreign remittance department zenith international bank plc writing letter ask support cooperation carry business opportunity department discovered abandoned sum fifteen million united states dollars account belongs one foreign customers died along entire family wife two children november plane crash since heard death expecting nextofkin come put claims money heirbecause cannot release fund account unless someone applies claim nextofkin deceased indicated banking guidelines unfortunately neither family member distant relative ever appeared claim said fund upon discoveryi officials department agreed make business release total amount account heir fund since one came discovered maintained account bank otherwise fund returned banks treasury unclaimed fund agreed ratio sharing stated thus foreign partner us officials department settlement local foreign expences incurred us course business upon successful completion transfer one colleagues come country mind share intend import agricultural machineries country way recycling fund commence transaction require immediately indicate interest return email enclose private contact telephone number fax number full name address designated bank coordinates enable us file letter claim appropriate departments necessary approvals transfer made note also transaction must kept strictly confidential nature look forward receiving prompt response regards mr mike chukwu zenith international bank plc'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pattern = r\"\\b(\" + \"|\".join(map(re.escape, stop_words)) + r\")\\b\"\n",
    "    return re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "X_train = X_train.apply(remove_words)\n",
    "X_test = X_test.apply(remove_words)\n",
    "\n",
    "X_train = X_train.apply(normalize_spaces)       # delete double spaces again\n",
    "X_test = X_test.apply(normalize_spaces)       # delete double spaces again\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     regards mr nelson smithkindly reply private em...\n",
       "535           able reach oscar supposed send pdb receive\n",
       "695    huma abedin bim checking pat work jack jake re...\n",
       "557                          announced monday cant today\n",
       "836    bank africaagence san pedro bp san pedro cote ...\n",
       "596    dear friendmy proposal surprising personal con...\n",
       "165    soneri bank limitedsilver jubilee centercbritt...\n",
       "918            easy joy powerful yahoo bookmarks toolbar\n",
       "495    amrecollince addoattn ai need urgent assistanc...\n",
       "824    dear nancyi much want meet weve trying pin sch...\n",
       "65     met fr gudziak come back kyiv visitalso john t...\n",
       "141             word back chinese availabilityblair fine\n",
       "925                                       steinberg call\n",
       "827                                       august hillary\n",
       "655    miss maria deniseaddress ave rue abidjan cote ...\n",
       "331    dear friende mre phil ipenzac provincial manag...\n",
       "664    permission changing statement spokesmans name ...\n",
       "249    dear friendi dr yetunde bassey bank manager di...\n",
       "907    also ann sending number people include kc hono...\n",
       "708    complimentc contact surprising mannerc respect...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'permission changing statement spokesmans name rather makes genericstatement us policy holds reservewe tweaked first paragraph make clear expectationon mexico consensus call worthwhile get updated call sheet accounts interveningeventsthe remaining honduras related call sheets also coming wayplease let know help anything else'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=664\n",
    "X_train[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'permission change statement spokesman name rather make genericstatement u policy hold reservewe tweak first paragraph make clear expectationon mexico consensus call worthwhile get update call sheet account interveningeventsthe remain honduras relate call sheet also come wayplease let know help anything else'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def to_wordnet_pos(treebank_tag: str):\n",
    "    if not treebank_tag:\n",
    "        return wordnet.NOUN\n",
    "    t = treebank_tag[0]\n",
    "    return {\n",
    "        'J': wordnet.ADJ,   # JJ, JJR, JJS\n",
    "        'V': wordnet.VERB,  # VB, VBD, VBG, VBN, VBP, VBZ\n",
    "        'N': wordnet.NOUN,  # NN, NNS, NNP, NNPS\n",
    "        'R': wordnet.ADV    # RB, RBR, RBS\n",
    "    }.get(t, wordnet.NOUN)\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_with_pos(text: str) -> str:\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged  = pos_tag(tokens)\n",
    "    lemmas = [lemma.lemmatize(tok, pos=to_wordnet_pos(tag)) for tok, tag in tagged]\n",
    "    return re.sub(r\"\\s+\", \" \", \" \".join(lemmas)).strip()\n",
    "\n",
    "X_train = X_train.apply(lemmatize_with_pos)\n",
    "X_train[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
